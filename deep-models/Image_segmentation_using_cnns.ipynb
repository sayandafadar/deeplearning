{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Three esstential computer vision tasks\n\n* Image classification\n* Image segmentation\n* Object detection\n\nDeep learning for computer vision also encompasses a number of somewhat more niche tasks besides these three, such as image similarity scoring (estimating how visually similar two images are), keypoint detection (pinpointing attributes of interest in an image, such as facial features), pose estimation, 3D mesh estimation, and so on.","metadata":{"id":"XkyoFQpqcoBh"}},{"cell_type":"markdown","source":"## An image segmentation example\n\nImage segmentation with deep learning is about using a model to assign a class to each pixel in an image, thus segmenting the image into different zones (such as \"background\" and \"foreground\", or \"road\", \"car\", and \"sidewalk\").\n\nThere are two different flavors of image segmentation:\n* *Semantic segmentation,* where each pixel is independently classified into semantic category, like \"cat\". If there are two cats in the image, the corresponding pixels are all mapped to the same generic \"cat\" category.\n* *Instance segmentation,* which seeks not only to classify image pixels by category, but alse to parse out individual object instances. In an image with two cats in it, instance segmentation would treat \"cat 1\" and \"cat 2\" as two separate classes of pixels.\n\nA ***segmentation*** mask is the image-segmentation equivalent of a label.","metadata":{"id":"TazsOSTXgqys"}},{"cell_type":"code","source":"# Downloading and uncompressing our dataset\n!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n!tar -xf images.tar.gz\n!tar -xf annotations.tar.gz","metadata":{"id":"qjMFBASKbkSJ","outputId":"d6eb6464-7751-47ba-e912-b53b6d9e12c8","execution":{"iopub.status.busy":"2023-01-31T14:28:42.355499Z","iopub.execute_input":"2023-01-31T14:28:42.355959Z","iopub.status.idle":"2023-01-31T14:29:34.272559Z","shell.execute_reply.started":"2023-01-31T14:28:42.355869Z","shell.execute_reply":"2023-01-31T14:29:34.271252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's prepare the list of input file paths, as well as the list of the corresponding mask file paths:\nimport os\n\ninput_dir = \"images/\"\ntarget_dir = \"annotations/trimaps\"\n\ninput_img_paths = sorted([\n    os.path.join(input_dir, fname)\n    for fname in os.listdir(input_dir)\n    if fname.endswith(\".jpg\")])\n\ntarget_paths = sorted(\n    [os.path.join(target_dir, fname)\n    for fname in os.listdir(target_dir)\n    if fname.endswith(\".png\") and not fname.startswith(\".\")])","metadata":{"id":"b-EusuWrmaAK","execution":{"iopub.status.busy":"2023-01-31T14:29:34.274780Z","iopub.execute_input":"2023-01-31T14:29:34.275097Z","iopub.status.idle":"2023-01-31T14:29:34.324228Z","shell.execute_reply.started":"2023-01-31T14:29:34.275066Z","shell.execute_reply":"2023-01-31T14:29:34.323346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, what does one of these inputs and its mask look like?\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import load_img, img_to_array\n\nplt.axis(\"off\") \nplt.imshow(load_img(input_img_paths[9])); # Display inut image number 9","metadata":{"id":"nrc_lcIon1an","outputId":"3246a1e8-0234-4370-ffb5-5c3ba9f0b370","execution":{"iopub.status.busy":"2023-01-31T14:29:34.325527Z","iopub.execute_input":"2023-01-31T14:29:34.325965Z","iopub.status.idle":"2023-01-31T14:29:40.113455Z","shell.execute_reply.started":"2023-01-31T14:29:34.325927Z","shell.execute_reply":"2023-01-31T14:29:40.112410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# And here is it's correnponding target\ndef display_target(target_array):\n  normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n  plt.axis(\"off\")\n  plt.imshow(normalized_array[:, :, 0])\n\nimg = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\ndisplay_target(img)","metadata":{"id":"eAzcM3GuosUc","outputId":"38f22956-7c8b-42f4-e7b2-09ccbdde15c6","execution":{"iopub.status.busy":"2023-01-31T14:29:40.115681Z","iopub.execute_input":"2023-01-31T14:29:40.117347Z","iopub.status.idle":"2023-01-31T14:29:40.262215Z","shell.execute_reply.started":"2023-01-31T14:29:40.117305Z","shell.execute_reply":"2023-01-31T14:29:40.260712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_img_paths[:5], target_paths[:5]","metadata":{"id":"LXVubuN99N2n","outputId":"1b2869c5-1519-423b-8d42-efeddc37f441","execution":{"iopub.status.busy":"2023-01-31T14:29:40.269223Z","iopub.execute_input":"2023-01-31T14:29:40.272929Z","iopub.status.idle":"2023-01-31T14:29:40.288211Z","shell.execute_reply.started":"2023-01-31T14:29:40.272856Z","shell.execute_reply":"2023-01-31T14:29:40.287052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom tensorflow.keras.utils import load_img, img_to_array\n\n# Setting the img_size\nimg_size = (200, 200) # A 200x200 square img\nnum_imgs = len(input_img_paths) # Total number of img paths downloaded from the dataset\n\n# Shuffling the paths so that the CNN can learn \n# at random and doesn't form some particular patterns\nrandom.Random(1337).shuffle(input_img_paths)\nrandom.Random(1337).shuffle(target_paths)\n\ndef path_to_input_image(path):\n  \"\"\"\n  Transform an image from it's reference path in disk to an \n  actual array of numbers, because DNNs only learn with numbers\n  and not actual images.\n  \"\"\"\n  return img_to_array(load_img(path, target_size=img_size))\n\ndef path_to_target(path):\n  \"\"\"\n  Transform a target image from it's reference path in disk\n  to an actual array of numbers, because DNNs only learn with \n  numbers and not actual images.\n  \"\"\"\n  img = img_to_array(\n      load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n  img = img.astype(\"uint8\") - 1 \n  return img\n\n# Initializing the initial input_imgs tensor with (7390, 200, 200, 3) shape\ninput_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\ntargets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n\n# Setting the actual values of the image array to the inputs_imgs\nfor i in range(num_imgs):\n  input_imgs[i] = path_to_input_image(input_img_paths[i])\n  targets[i] = path_to_target(target_paths[i])\n\n\nnum_val_samples = 1000\ntrain_input_imgs = input_imgs[:-num_val_samples]\ntrain_targets = targets[:-num_val_samples]\nval_input_imgs = input_imgs[-num_val_samples:]\nval_targets = targets[-num_val_samples:]","metadata":{"id":"ExQObk3C49qS","execution":{"iopub.status.busy":"2023-01-31T14:29:40.290481Z","iopub.execute_input":"2023-01-31T14:29:40.291086Z","iopub.status.idle":"2023-01-31T14:30:24.563592Z","shell.execute_reply.started":"2023-01-31T14:29:40.291039Z","shell.execute_reply":"2023-01-31T14:30:24.562577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Convolution** decreases the size of the input layer.\n* When convulating an input of size `(200x200)` with a `kernel_size` of `3x3` it will result in `(198x198)`.\n* When convulating an input of size `(200x200)` with a `kernel_size` of `5x5` it will result in `(196x196)`.\n\nBut by setting the `padding=\"same\"` the size after the convolution doesn't change.\n* A `(200x200)` size input convoluted with a `kernel_size` of `3x3` and with a `padding=\"same\"` results with an output size of `(200x200)`.","metadata":{"id":"Sn8rg4c2HK38"}},{"cell_type":"code","source":"# Now it's time to define our model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model(img_size, num_classes):\n  inputs = keras.Input(shape=img_size + (3,))\n  x = layers.Rescaling(1./255)(inputs)\n\n  x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n  x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n\n  x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n  x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n  x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n  x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n\n  outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n  model = keras.Model(inputs, outputs)\n  return model","metadata":{"id":"jh-BNKYw_wrp","execution":{"iopub.status.busy":"2023-01-31T14:30:24.564950Z","iopub.execute_input":"2023-01-31T14:30:24.565580Z","iopub.status.idle":"2023-01-31T14:30:24.577532Z","shell.execute_reply.started":"2023-01-31T14:30:24.565541Z","shell.execute_reply":"2023-01-31T14:30:24.576471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(img_size=img_size, num_classes=3)\nmodel.summary()","metadata":{"id":"7z-HGF66NBDd","outputId":"4d23fe3d-4398-412f-f68e-4b706d9dcbb7","execution":{"iopub.status.busy":"2023-01-31T14:30:24.580079Z","iopub.execute_input":"2023-01-31T14:30:24.580863Z","iopub.status.idle":"2023-01-31T14:30:27.534318Z","shell.execute_reply.started":"2023-01-31T14:30:24.580823Z","shell.execute_reply":"2023-01-31T14:30:27.533330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Why we used strides to downsample instead of a MaxPooling2D layer?\n\n>We do this because, in the case of image segmentation, we care a lot about the *spatial location* of information in the image, since we need to produce per-pixel target masks as output of the model. When you do 2 x 2 maxpooling, you are completely destroying location information within each pooling window: you return one scalar value per window, with zero knowledge of which of the four locations in the windows the value came from. So while max pooling layers perform well for classification tasks, they would hurt quite a bit for a segmentation task. Meanwhile, strided convolutions do a better job at downsampling feature maps while retaining location information.","metadata":{"id":"PWg6F1GPJ5lX"}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\")\n\n# Make the callbacks\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n                                    save_best_only=True)\n]\n\n# Fit the model\nhistory = model.fit(train_input_imgs, train_targets,\n                    epochs=50,\n                    callbacks=callbacks,\n                    batch_size=64,\n                    validation_data=(val_input_imgs, val_targets))","metadata":{"id":"XoTPuxgEFSvi","outputId":"812a2ab5-74ef-4051-c14f-407b1f3c4392","execution":{"iopub.status.busy":"2023-01-31T14:30:27.535992Z","iopub.execute_input":"2023-01-31T14:30:27.536417Z","iopub.status.idle":"2023-01-31T14:58:57.135984Z","shell.execute_reply.started":"2023-01-31T14:30:27.536378Z","shell.execute_reply":"2023-01-31T14:58:57.134804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(history.history[\"loss\"]) + 1)\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend();","metadata":{"id":"w_afT-aHFgOw","execution":{"iopub.status.busy":"2023-01-31T15:09:41.282325Z","iopub.execute_input":"2023-01-31T15:09:41.282784Z","iopub.status.idle":"2023-01-31T15:09:41.493414Z","shell.execute_reply.started":"2023-01-31T15:09:41.282738Z","shell.execute_reply":"2023-01-31T15:09:41.492491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import array_to_img\n\nmodel = keras.models.load_model(\"oxford_segmentation.keras\")\n\ni = 20\ntest_image = val_input_imgs[i]\nplt.axis(\"off\")\nplt.imshow(array_to_img(test_image))\nplt.show()\n\nmask = model.predict(np.expand_dims(test_image, 0))[0]\n\ndef display_mask(pred):\n    mask = np.argmax(pred, axis=-1)\n    mask *= 127\n    plt.axis(\"off\")\n    plt.imshow(mask);\n    \ndisplay_mask(mask)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T15:23:45.712410Z","iopub.execute_input":"2023-01-31T15:23:45.712773Z","iopub.status.idle":"2023-01-31T15:23:46.357759Z","shell.execute_reply.started":"2023-01-31T15:23:45.712738Z","shell.execute_reply":"2023-01-31T15:23:46.356453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modern convnet architecture pattterns\n\nA model's \"architecture\" is the sum of the choices that went into creating itL which layers to use, how to configure them, and in what arrangement to connect them. These choices define the *hypothesis space* of your model: the space of possible functions that gradient descent can search over, parameterized by the model's weights.\n\nModel architecture is often the difference between success and failure. If you make inappropriate choices, your model may be stuck with suboptimal metrics, and amount of training data will save it. Inversely, a good model architecture will accelerate learning and will enable your model to make efficient use of the training data available, reducing the need for large datasets. A good model architecture is one that reduces the size of the search space or otherwise make it easier to converge to a good point of the search space. Just like feature engineering and data curation, model architecture is all about making the problem simpler for gradient descent to solve. And remember that gradient descent is a pretty stupid search process, so it needs all the help it can get.","metadata":{}},{"cell_type":"markdown","source":"### Modularity, hierarchy, and resue\n\nIf you want to make a complex system simpler, there's a universal recipe you can apply: just structure your amorphous soup of complexity into *modules*, organize the modules into a *hierarchy*, and start *reusing* the same modules in multiple places as appropriate (\"reuse\" is another word for *abstraction* in this context).","metadata":{}},{"cell_type":"markdown","source":"### Residual connections\n\n`y = f4(f3(f2(f1(x))))`\n\nTo adjust `f1`, you'll need to percolate error information through `f2`, `f3`, and `f4`. However, each successive function in the chain introduces some amount of noise. If your function chain is too deep, this noise starts overwhelming gradient information, and backpropagation stops working. Your model won't train at all. This is the ***vanishing gradients*** problem.\n\nThe fix is simple: just force each function in the chain to be nondestructive - to retain a noiseless version of the information contained in the previous input. The easiest way to implement this is to use a *residual connection*. It's dead easy: just add the input of a layer or block of layers back to its output. The *residual* connection acts as in *information shorcut* around destructive or noisy blocks (such as blocks that contain `relu` activations or `dropout` layers), enabling error gradient information from early layers to propagate noiselessly through a deep network.\n\n![image.png](attachment:ff6ba0c6-2df0-440b-9138-a481bb87103f.png)","metadata":{},"attachments":{"ff6ba0c6-2df0-440b-9138-a481bb87103f.png":{"image/png":"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"}}},{"cell_type":"code","source":"# A residual connection in pseudocode\nx = \"some input\"\nresidual = x # Save a pointer to the original input. This is called the residual\nx = block(x) # This computation block can potentially be destructive or noisy, and that's fine\nx = add([x, residual]) # Add the original input to the layer's output: the final output will thus always preserve full information about the original input.","metadata":{"execution":{"iopub.status.busy":"2023-02-01T05:04:57.316535Z","iopub.execute_input":"2023-02-01T05:04:57.316926Z","iopub.status.idle":"2023-02-01T05:04:57.321371Z","shell.execute_reply.started":"2023-02-01T05:04:57.316892Z","shell.execute_reply":"2023-02-01T05:04:57.320392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">**Note:** Adding the input back to the output of a block implies that the output should have the same shape as the input. However, this is not the case if your block includes convolutional layers with an increased number of filters, or a max pooling layer. In such cases, use a `1 x 1 Conv2D` layer with no activation to linearly project the residual to the deired output shape. You'd typically use `padding=\"same\"` in the convolution layers in your target block so as to avoid spatial downsampling due to padding, and you'd use strides in the residual projection to match and downsampling caused by a max pooling layer.","metadata":{}},{"cell_type":"code","source":"# Residual block where the number of filters changes\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ninputs = keras.Input(shape=(32, 32, 3)) \nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\nresidual = x # Set aside the residual\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) # This is the layer around which we create a residual connection: it increases the number of output filters from 32 to 64. Note that we use padding=\"same\" to avoid downsampling due to padding.\nresidual = layers.Conv2D(64, 1)(residual) # The residual only had 32 filters, so we use a 1 x 1 Conv2D to project it to the correct shape.\nx = layers.add([x, residual])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T09:09:21.291872Z","iopub.execute_input":"2023-02-01T09:09:21.292225Z","iopub.status.idle":"2023-02-01T09:09:29.991305Z","shell.execute_reply.started":"2023-02-01T09:09:21.292195Z","shell.execute_reply":"2023-02-01T09:09:29.990288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Case where the target block includes a max pooling layer\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\nresidual = x # Set aside the residual\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) \nx = layers.MaxPooling2D(2, padding=\"same\")(x) # This is the block of two layers around which we create a residual connection: it includes a 2 x 2 max pooling layer. Note that we use padding=\"same\" in both the convolution layer and the max pooling layer to avoid downsampling due to padding.\nresidual = layers.Conv2D(64, 1, strides=2)(residual) # We use strides=2 in the residual projection to match the downsampling created by the max pooling layer\nx = layers.add([x, residual]) # Now the block output and the residual have the same shape and can be added.","metadata":{"execution":{"iopub.status.busy":"2023-02-01T09:09:29.993307Z","iopub.execute_input":"2023-02-01T09:09:29.993952Z","iopub.status.idle":"2023-02-01T09:09:30.032675Z","shell.execute_reply.started":"2023-02-01T09:09:29.993915Z","shell.execute_reply":"2023-02-01T09:09:30.031594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A concrete example\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Rescaling(1./255)(inputs) \n\ndef residual_block(x, filters, pooling=False): # Utitlity function to apply a convolution block with a residual connection, with an option to add max pooing\n    residual = x\n    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n    \n    if pooling:\n        x = layers.MaxPooling2D(2, padding=\"same\")(x) \n        residual = layers.Conv2D(filters, 1, strides=2)(residual) # If we use max pooling, we add a strided convolution to project the residual to the expected shape\n    elif filters != residual.shape[-1]:\n        residual = layers.Conv2D(filters, 1)(residual) # If we don't use max pooling, we only project the residual if the number of channels has changed\n    x = layers.add([x, residual])\n    return x\n\nx = residual_block(x, filters=32, pooling=True) # First block\nx = residual_block(x, filters=64, pooling=True) # Second block; note the increasing filter count in each block\nx = residual_block(x, filters=128, pooling=False) # The last block doesn't need a max pooling layer, since we will apply global pooling right after it.\n\nx = layers.GlobalAveragePooling2D()(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T09:09:30.137168Z","iopub.execute_input":"2023-02-01T09:09:30.137800Z","iopub.status.idle":"2023-02-01T09:09:30.389516Z","shell.execute_reply.started":"2023-02-01T09:09:30.137763Z","shell.execute_reply":"2023-02-01T09:09:30.388497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With residual connections, you can build networks of arbitrary depth, without having to worry about vanishing gradients.","metadata":{}},{"cell_type":"markdown","source":"### Batch Normalization\n\nNormalization is a broad category of methods seek to make different samples seen by a machine learning model more similar to each other, which helps the model learn and generalize well to new data.\n\n>**Batch normalization** is a type of layer which can adaptively normalize data even as the mean and variance change over time during training. During training, it uses the mean and variance of the current batch of data to normalize samples, and during inference (when a big enough batch of representative data may not be available), it uses an exponential moving average of the batch-wise mean and variance of the data seen during training.","metadata":{}},{"cell_type":"code","source":"# How to use batch normalization: no bias vector and the activation comes last\nx = layers.Conv2D(32, 3, use_bias=False)(x) # Note the lack of activation here\nx = layers.BatchNormalization()(x)\nx = layers.Activation(\"relu\")(x) # We place the activation after the BatchNormalizartion layer","metadata":{"execution":{"iopub.status.busy":"2023-02-01T09:09:46.006585Z","iopub.execute_input":"2023-02-01T09:09:46.006949Z","iopub.status.idle":"2023-02-01T09:09:46.011311Z","shell.execute_reply.started":"2023-02-01T09:09:46.006921Z","shell.execute_reply":"2023-02-01T09:09:46.010357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The convnet architecture\n\n* Your model should be organized into repeated *blocks*of layers, usually made of multiple convolution layers and a max pooling layer.\n* The number of filters in your layers should increase as the size of the spatial feature maps decreases.\n* Deep and narrow is better than broas and shallow.\n* Introducing residual connections and blocks of layers helps you train deeper networks.\n* It can be beneficial to introduce batch normalization layers after your convolution layers.\n* It can be beneficial to replace Conv2D layers with SeparableConv2D layers, which are more parameter-efficient.","metadata":{}},{"cell_type":"markdown","source":"## Putting it together: A mini Xception-like model","metadata":{}},{"cell_type":"code","source":"# Data augmentation \ndata_augmentation = keras.Sequential([\n layers.RandomFlip(\"horizontal\"),\n layers.RandomRotation(0.1),\n layers.RandomZoom(0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T09:09:53.884801Z","iopub.execute_input":"2023-02-01T09:09:53.885234Z","iopub.status.idle":"2023-02-01T09:09:53.920599Z","shell.execute_reply.started":"2023-02-01T09:09:53.885195Z","shell.execute_reply":"2023-02-01T09:09:53.919415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = keras.Input(shape=(180, 180, 3)) \nx = data_augmentation(inputs)\n\nx = layers.Rescaling(1./255)(x) \nx = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n\nfor size in [32, 64, 128, 256, 512]: \n    residual = x\n    \n    x = layers.BatchNormalization()(x) \n    x = layers.Activation(\"relu\")(x) \n    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n    \n    x = layers.BatchNormalization()(x) \n    x = layers.Activation(\"relu\")(x) \n    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n    \n    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n    \n    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n    \n    x = layers.add([x, residual])\n    \nx = layers.GlobalAveragePooling2D()(x) \nx = layers.Dropout(0.5)(x) \noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T09:12:38.179418Z","iopub.execute_input":"2023-02-01T09:12:38.180377Z","iopub.status.idle":"2023-02-01T09:12:38.542704Z","shell.execute_reply.started":"2023-02-01T09:12:38.180340Z","shell.execute_reply":"2023-02-01T09:12:38.541750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpreting what convnets learn\n\nVisualization of how different parts of convnets:\n* *Visualizing intermediate convnet outputs (intermediat activations)* - Useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.\n* *Visualizing convnet filters* - Useful for understanding precisely what visual pattern or concept each filter in a convnet is receptive to.\n* *Visualizing heatmaps of class activation in an image* - Useful for understanding which parts of an image were identified as belonging to a given class, thus allowing you to localize objects in images.","metadata":{}},{"cell_type":"markdown","source":"### Visualizing intermediate activations\n\nVisualizing intermediate activations consists of displaying the values returned by various convolution and pooling layers in a model, given a certain input(the output of a layer is often called its activation, the output of the activation function). \n\nEach channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image.","metadata":{}},{"cell_type":"code","source":"# Reloading the model\nfrom tensorflow import keras\nmodel = keras.models.load_model(\"/kaggle/input/convnet-from-scratch-with-augmentationkeras/convnet_from_scratch_with_augmentation.keras\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:26:56.765973Z","iopub.execute_input":"2023-02-01T11:26:56.766371Z","iopub.status.idle":"2023-02-01T11:26:57.257835Z","shell.execute_reply.started":"2023-02-01T11:26:56.766336Z","shell.execute_reply":"2023-02-01T11:26:57.256680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll get an input image - apicture of a cat, not part of the images the network was trained on.","metadata":{}},{"cell_type":"code","source":"# Proprocessing a single image\nfrom tensorflow import keras\nimport numpy as np\n\n# Download a test image\nimg_path = keras.utils.get_file(\n    fname=\"cat.jpg\",\n    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n\ndef get_img_array(img_path, target_size):\n    img = keras.utils.load_img(\n        img_path, target_size=target_size) # Open the image and resize it\n    array = keras.utils.img_to_array(img) # Turn the image into a float32 Numpy array of shape (180, 180, 3)\n    array = np.expand_dims(array, axis=0) # Add a dimension to transform the array into a \"batch\" of a single sample. Its shape is now (1, 180, 180, 3)\n    return array\n\nimg_tensor = get_img_array(img_path, target_size=(180, 180))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:28:56.492292Z","iopub.execute_input":"2023-02-01T11:28:56.492699Z","iopub.status.idle":"2023-02-01T11:28:57.221615Z","shell.execute_reply.started":"2023-02-01T11:28:56.492666Z","shell.execute_reply":"2023-02-01T11:28:57.220520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_tensor.size","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:29:39.747496Z","iopub.execute_input":"2023-02-01T11:29:39.747867Z","iopub.status.idle":"2023-02-01T11:29:39.755139Z","shell.execute_reply.started":"2023-02-01T11:29:39.747837Z","shell.execute_reply":"2023-02-01T11:29:39.754005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the test picture\nimport matplotlib.pyplot as plt\nplt.axis(\"off\")\nplt.imshow(img_tensor[0].astype(\"uint8\"))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:29:59.634796Z","iopub.execute_input":"2023-02-01T11:29:59.635659Z","iopub.status.idle":"2023-02-01T11:29:59.777917Z","shell.execute_reply.started":"2023-02-01T11:29:59.635622Z","shell.execute_reply":"2023-02-01T11:29:59.776636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to extract the feature maps we want to look at, we'll create a Keras model that takes batches of images as input, and that outputs the activations of all convolution and pooling layers.","metadata":{}},{"cell_type":"code","source":"# Instantialting a model that returns layer activations\nfrom tensorflow.keras import layers\n\nlayer_outputs = [] \nlayer_names = [] \nfor layer in model.layers: \n    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)): \n        layer_outputs.append(layer.output)\n        layer_names.append(layer.name)\nactivation_model = keras.Model(inputs=model.input, outputs=layer_outputs)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:32:03.672194Z","iopub.execute_input":"2023-02-01T11:32:03.672623Z","iopub.status.idle":"2023-02-01T11:32:03.684629Z","shell.execute_reply.started":"2023-02-01T11:32:03.672585Z","shell.execute_reply":"2023-02-01T11:32:03.683508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the model to compute layer activations\nactivations = activation_model.predict(img_tensor)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:33:37.723921Z","iopub.execute_input":"2023-02-01T11:33:37.724811Z","iopub.status.idle":"2023-02-01T11:33:37.806874Z","shell.execute_reply.started":"2023-02-01T11:33:37.724759Z","shell.execute_reply":"2023-02-01T11:33:37.805917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for activation in activations:\n    print(activation.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:34:18.721545Z","iopub.execute_input":"2023-02-01T11:34:18.721942Z","iopub.status.idle":"2023-02-01T11:34:18.727850Z","shell.execute_reply.started":"2023-02-01T11:34:18.721910Z","shell.execute_reply":"2023-02-01T11:34:18.726697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_layer_activation = activations[0]\nprint(first_layer_activation.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:35:22.554295Z","iopub.execute_input":"2023-02-01T11:35:22.554666Z","iopub.status.idle":"2023-02-01T11:35:22.560163Z","shell.execute_reply.started":"2023-02-01T11:35:22.554637Z","shell.execute_reply":"2023-02-01T11:35:22.559347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_layer_activation[0, :, :, 5]","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:36:17.558075Z","iopub.execute_input":"2023-02-01T11:36:17.558458Z","iopub.status.idle":"2023-02-01T11:36:17.565940Z","shell.execute_reply.started":"2023-02-01T11:36:17.558427Z","shell.execute_reply":"2023-02-01T11:36:17.565093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the fifth channel\nplt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\");","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:36:24.729500Z","iopub.execute_input":"2023-02-01T11:36:24.729877Z","iopub.status.idle":"2023-02-01T11:36:24.987622Z","shell.execute_reply.started":"2023-02-01T11:36:24.729844Z","shell.execute_reply":"2023-02-01T11:36:24.986379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(32):\n    plt.matshow(first_layer_activation[0, :, :, i], cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:37:34.321787Z","iopub.execute_input":"2023-02-01T11:37:34.323249Z","iopub.status.idle":"2023-02-01T11:37:43.054087Z","shell.execute_reply.started":"2023-02-01T11:37:34.323188Z","shell.execute_reply":"2023-02-01T11:37:43.052902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's plot a complete visualization of all the activations in the network. We'll extract and plot every channel in each of the layer activations, and we'll stack the results in one big grid, with channels stacked side by side.","metadata":{}},{"cell_type":"code","source":"# Visualizing every channel in every intermediate activation\nimages_per_row = 16\nfor layer_name, layer_activation in zip(layer_names, activations):\n    n_features = layer_activation.shape[-1]\n    size = layer_activation.shape[1]\n    n_cols = n_features // images_per_row\n    display_grid = np.zeros(((size + 1) * n_cols - 1,\n                            images_per_row * (size + 1) - 1))\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_index = col * images_per_row + row\n            channel_image = layer_activation[0, :, :, channel_index].copy()\n            if channel_image.sum() != 0:\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n            display_grid[\n                col * (size + 1): (col + 1) * size + col, \n                row* (size + 1) : (row + 1) * size + row] = channel_image\n    scale = 1./size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                       scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.axis(\"off\")\n    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:01:08.462713Z","iopub.execute_input":"2023-02-01T12:01:08.463142Z","iopub.status.idle":"2023-02-01T12:01:10.736887Z","shell.execute_reply.started":"2023-02-01T12:01:08.463105Z","shell.execute_reply":"2023-02-01T12:01:10.735763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing convnet filters\n","metadata":{}},{"cell_type":"code","source":"# Instantiating the Xception convoltional base\nmodel = keras.applications.xception.Xception(\n    weights=\"imagenet\", \n    include_top=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:13:27.235551Z","iopub.execute_input":"2023-02-01T12:13:27.235990Z","iopub.status.idle":"2023-02-01T12:13:31.566021Z","shell.execute_reply.started":"2023-02-01T12:13:27.235941Z","shell.execute_reply":"2023-02-01T12:13:31.564579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the names of all convolutional layers in Xception\nfor layer in model.layers:\n    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n        print(layer.name)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:15:35.892969Z","iopub.execute_input":"2023-02-01T12:15:35.893405Z","iopub.status.idle":"2023-02-01T12:15:35.900594Z","shell.execute_reply.started":"2023-02-01T12:15:35.893371Z","shell.execute_reply":"2023-02-01T12:15:35.899341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a feature extractor model\nlayer_name = \"block3_sepconv1\"\nlayer = model.get_layer(name=layer_name)\nfeature_extractor = keras.Model(inputs=model.input, outputs=layer.output)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:18:39.764054Z","iopub.execute_input":"2023-02-01T12:18:39.764448Z","iopub.status.idle":"2023-02-01T12:18:39.774773Z","shell.execute_reply.started":"2023-02-01T12:18:39.764416Z","shell.execute_reply":"2023-02-01T12:18:39.773743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the feature extractor\nactivation = feature_extractor(\nkeras.applications.xception.preprocess_input(img_tensor))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:20:54.909646Z","iopub.execute_input":"2023-02-01T12:20:54.910068Z","iopub.status.idle":"2023-02-01T12:20:55.003533Z","shell.execute_reply.started":"2023-02-01T12:20:54.910031Z","shell.execute_reply":"2023-02-01T12:20:55.002325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef compute_loss(image, filter_index):\n    activation = feature_extractor(image)\n    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n    return tf.reduce_mean(filter_activation)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:24:13.654214Z","iopub.execute_input":"2023-02-01T12:24:13.654631Z","iopub.status.idle":"2023-02-01T12:24:13.660853Z","shell.execute_reply.started":"2023-02-01T12:24:13.654591Z","shell.execute_reply":"2023-02-01T12:24:13.659813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss maximization via stochastic gradient ascent\n@tf.function\ndef gradient_ascent_step(image, filter_index, learning_rate):\n    with tf.GradientTape() as tape:\n        tape.watch(image)\n        loss = compute_loss(image, filter_index)\n    grads = tape.gradient(loss, image)\n    grads = tf.math.l2_normalize(grads)\n    image += learning_rate * grads\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:31:56.676571Z","iopub.execute_input":"2023-02-01T12:31:56.677622Z","iopub.status.idle":"2023-02-01T12:31:56.685288Z","shell.execute_reply.started":"2023-02-01T12:31:56.677576Z","shell.execute_reply":"2023-02-01T12:31:56.684004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate filter visualizations\nimg_width = 200\nimg_height = 200\n\ndef generate_filter_pattern(filter_index):\n    iterations = 30\n    learning_rate = 10.\n    image = tf.random.uniform(\n        minval=0.4, \n        maxval=0.6, \n        shape=(1, img_width, img_height, 3))\n    for i in range(iterations):\n        image = gradient_ascent_step(image, filter_index, learning_rate)\n    return image[0].numpy()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:36:15.160103Z","iopub.execute_input":"2023-02-01T12:36:15.160581Z","iopub.status.idle":"2023-02-01T12:36:15.168824Z","shell.execute_reply.started":"2023-02-01T12:36:15.160534Z","shell.execute_reply":"2023-02-01T12:36:15.167521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utitlity function to convert a tensor into a valid image\ndef deprocess_image(image):\n    image -= image.mean()\n    image /= image.std()\n    image *= 64\n    image += 128\n    image = np.clip(image, 0, 255).astype(\"uint8\")\n    image = image[25:-25, 25:-25, :]\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:39:36.050458Z","iopub.execute_input":"2023-02-01T12:39:36.050838Z","iopub.status.idle":"2023-02-01T12:39:36.057264Z","shell.execute_reply.started":"2023-02-01T12:39:36.050806Z","shell.execute_reply":"2023-02-01T12:39:36.056316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis(\"off\")\nplt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)));","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:40:05.781820Z","iopub.execute_input":"2023-02-01T12:40:05.782268Z","iopub.status.idle":"2023-02-01T12:40:06.797519Z","shell.execute_reply.started":"2023-02-01T12:40:05.782230Z","shell.execute_reply":"2023-02-01T12:40:06.796247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing heatmaps of class activation","metadata":{}},{"cell_type":"code","source":"# Loading the Xception network with pretrained weights\nfrom tensorflow import keras\nfrom keras import layers\nmodel = keras.applications.xception.Xception(weights=\"imagenet\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:02:28.025973Z","iopub.execute_input":"2023-02-01T14:02:28.026411Z","iopub.status.idle":"2023-02-01T14:02:29.675661Z","shell.execute_reply.started":"2023-02-01T14:02:28.026373Z","shell.execute_reply":"2023-02-01T14:02:29.674232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing an input image for Xception\nimport numpy as np\nimg_path = keras.utils.get_file(\n    fname=\"elephant.jpg\", \n    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n\ndef get_img_array(img_path, target_size):\n    img = keras.utils.load_img(img_path, target_size=target_size)\n    array = keras.utils.img_to_array(img)\n    array = np.expand_dims(array, axis=0)\n    array = keras.applications.xception.preprocess_input(array)\n    return array\n\nimg_array = get_img_array(img_path, target_size=(299, 299))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:11:42.321468Z","iopub.execute_input":"2023-02-01T14:11:42.321907Z","iopub.status.idle":"2023-02-01T14:11:42.377598Z","shell.execute_reply.started":"2023-02-01T14:11:42.321869Z","shell.execute_reply":"2023-02-01T14:11:42.376380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:11:46.570038Z","iopub.execute_input":"2023-02-01T14:11:46.570462Z","iopub.status.idle":"2023-02-01T14:11:46.577836Z","shell.execute_reply.started":"2023-02-01T14:11:46.570426Z","shell.execute_reply":"2023-02-01T14:11:46.576726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array.size","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:11:58.827753Z","iopub.execute_input":"2023-02-01T14:11:58.828769Z","iopub.status.idle":"2023-02-01T14:11:58.837541Z","shell.execute_reply.started":"2023-02-01T14:11:58.828720Z","shell.execute_reply":"2023-02-01T14:11:58.836086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(img_array)\nprint(keras.applications.xception.decode_predictions(preds, top=3)[0])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:13:54.915383Z","iopub.execute_input":"2023-02-01T14:13:54.915782Z","iopub.status.idle":"2023-02-01T14:13:55.164464Z","shell.execute_reply.started":"2023-02-01T14:13:54.915750Z","shell.execute_reply":"2023-02-01T14:13:55.163256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up a model that returns the last convolutional output\nlast_conv_layer_name = \"block14_sepconv2_act\"\nclassifier_layer_names = [\n    \"avg_pool\",\n    \"predictions\"\n]\nlast_conv_layer = model.get_layer(last_conv_layer_name)\nlast_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:19:49.694324Z","iopub.execute_input":"2023-02-01T14:19:49.694759Z","iopub.status.idle":"2023-02-01T14:19:49.711757Z","shell.execute_reply.started":"2023-02-01T14:19:49.694722Z","shell.execute_reply":"2023-02-01T14:19:49.710273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reapplying the classifier on top of the last convolutional output\nclassifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\nx = classifier_input\nfor layer_name in classifier_layer_names:\n    x = model.get_layer(layer_name)(x)\nclassifier_model = keras.Model(classifier_input, x)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:22:08.361526Z","iopub.execute_input":"2023-02-01T14:22:08.362886Z","iopub.status.idle":"2023-02-01T14:22:08.382901Z","shell.execute_reply.started":"2023-02-01T14:22:08.362838Z","shell.execute_reply":"2023-02-01T14:22:08.381797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieving the gradients of the top predicted class\nimport tensorflow as tf\n\nwith tf.GradientTape() as tape:\n    last_conv_layer_output = last_conv_layer_model(img_array)\n    tape.watch(last_conv_layer_output)\n    preds = classifier_model(last_conv_layer_output)\n    top_pred_index = tf.argmax(preds[0])\n    top_class_channel = preds[:, top_pred_index]\n    \ngrads = tape.gradient(top_class_channel, last_conv_layer_output)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:25:44.372546Z","iopub.execute_input":"2023-02-01T14:25:44.373010Z","iopub.status.idle":"2023-02-01T14:25:45.798940Z","shell.execute_reply.started":"2023-02-01T14:25:44.372970Z","shell.execute_reply":"2023-02-01T14:25:45.797332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient pooling and channel-importance weighting\npooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy() \nlast_conv_layer_output = last_conv_layer_output[0] \nfor i in range(pooled_grads.shape[-1]): \n    last_conv_layer_output[:, :, i] *= pooled_grads[i]\nheatmap = np.mean(last_conv_layer_output, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T14:28:21.610112Z","iopub.execute_input":"2023-02-01T14:28:21.610546Z","iopub.status.idle":"2023-02-01T14:28:21.639994Z","shell.execute_reply.started":"2023-02-01T14:28:21.610508Z","shell.execute_reply":"2023-02-01T14:28:21.638414Z"},"trusted":true},"execution_count":null,"outputs":[]}]}