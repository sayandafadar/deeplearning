{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPxX9LsUWbpG9v0orIKWpWa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/davidADSP/Generative_Deep_Learning_2nd_Edition/main/notebooks/utils.py"],"metadata":{"id":"4gYbla5kiS7g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683262179861,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}},"outputId":"6f80ce58-d14c-4982-faec-3ff715ec679e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-05-05 04:49:50--  https://raw.githubusercontent.com/davidADSP/Generative_Deep_Learning_2nd_Edition/main/notebooks/utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 771 [text/plain]\n","Saving to: ‘utils.py’\n","\n","\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]     771  --.-KB/s    in 0s      \n","\n","2023-05-05 04:49:50 (30.7 MB/s) - ‘utils.py’ saved [771/771]\n","\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"l0WOb4bziCks","executionInfo":{"status":"ok","timestamp":1683262183769,"user_tz":-330,"elapsed":3911,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"outputs":[],"source":["import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras import (\n","    datasets,\n","    layers,\n","    models,\n","    optimizers,\n","    activations,\n","    metrics,\n","    callbacks,\n",")\n","\n","from utils import display, sample_batch\n","import random"]},{"cell_type":"code","source":["IMAGE_SIZE = 32\n","CHANNELS = 1\n","STEP_SIZE = 10\n","STEPS = 60\n","NOISE = 0.005\n","ALPHA = 0.1\n","GRADIENT_CLIP = 0.03\n","BATCH_SIZE = 128\n","BUFFER_SIZE = 8192\n","LEARNING_RATE = 0.0001\n","EPOCHS = 60"],"metadata":{"id":"2hN8AbmFiVpF","executionInfo":{"status":"ok","timestamp":1683262183771,"user_tz":-330,"elapsed":35,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Load the data\n","(x_train, _), (x_test, _) = datasets.mnist.load_data()"],"metadata":{"id":"KNBplzOriZcN","executionInfo":{"status":"ok","timestamp":1683262184473,"user_tz":-330,"elapsed":733,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd462a01-3c7f-4df2-b842-5be0598a65d2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# Preporcess the data\n","def preprocess(imgs):\n","  \"\"\"Normalize and reshape the images\"\"\"\n","  imgs = (imgs.astype(\"float32\") - 127.5) / 127.5\n","  imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=-1.0)\n","  imgs = np.expand_dims(imgs, -1)\n","  return imgs\n","\n","x_train = preprocess(x_train)\n","x_test = preprocess(x_test)"],"metadata":{"id":"y50_0LGyin4Z","executionInfo":{"status":"ok","timestamp":1683262185214,"user_tz":-330,"elapsed":744,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(BATCH_SIZE)\n","x_test = tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE)"],"metadata":{"id":"0cGRYCqzjQc9","executionInfo":{"status":"ok","timestamp":1683262191094,"user_tz":-330,"elapsed":5882,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## The Energy Function\n","The energy function $E_{\\theta}(x)$ is a neural network with parameters $\\theta$ that can transform an input image $x$ into a scalar value."],"metadata":{"id":"ejUFIbInkArm"}},{"cell_type":"markdown","source":["The network is a set of stacked `Conv2D` layers that gradually reduce the size of the image while increasing the number of channels. This final layer is a single fully connected unit with linear activation, so the network can output values in the range $(-∞, ∞)$."],"metadata":{"id":"-Gn80dvdlE2S"}},{"cell_type":"code","source":["ebm_input = layers.Input(shape=(32, 32, 1))\n","x = layers.Conv2D(16, kernel_size=5, strides=2, padding=\"same\", activation=activations.swish)(ebm_input)\n","x = layers.Conv2D(32, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish)(x)\n","x = layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish)(x)\n","x = layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish)(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(64, activation=activations.swish)(x)\n","ebm_output = layers.Dense(1)(x)\n","model = models.Model(ebm_input, ebm_output)"],"metadata":{"id":"haRAEkWnjwdZ","executionInfo":{"status":"ok","timestamp":1683262192430,"user_tz":-330,"elapsed":1338,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["##The Langevin sampling function"],"metadata":{"id":"BakxEgUfpAPN"}},{"cell_type":"code","source":["def generate_samples(model, inp_imgs, steps, step_size, noise, return_img_per_step=False):\n","  imgs_per_step = []\n","  for _ in range(steps):\n","    inp_imgs += tf.random.normal(inp_imgs.shape, mean=0, stddev=noise)\n","    inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n","    with tf.GradientTape() as tape:\n","      tape.watch(inp_imgs)\n","      out_score = model(inp_imgs)\n","    grads = tape.gradient(out_score, inp_imgs)\n","    grads = tf.clip_by_value(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n","    inp_imgs += step_size * grads\n","    inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n","    if return_img_per_step:\n","      imgs_per_step.append(inp_imgs)\n","  if return_img_per_step:\n","    return tf.stack(imgs_per_step, axis=0)\n","  else:\n","    return inp_imgs"],"metadata":{"id":"KLS8fyPkms7L","executionInfo":{"status":"ok","timestamp":1683262192431,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Buffer:\n","  def __init__(self, model):\n","    super().__init__()\n","    self.model = model\n","    self.examples = [\n","        tf.random.uniform(shape=(1, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2 -1\n","        for _ in range(128)\n","    ]\n","\n","  def sample_new_exmps(self, steps, step_size, noise):\n","    n_new = np.random.binomial(BATCH_SIZE, 0.05)\n","    rand_imgs = (\n","        tf.random.uniform((n_new, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2 - 1\n","    )\n","    old_imgs = tf.concat(\n","        random.choices(self.examples, k=BATCH_SIZE - n_new), axis=0\n","    )\n","    inp_imgs = tf.concat([rand_imgs, old_imgs], axis=0)\n","    inp_imgs = generate_samples(\n","        self.model, inp_imgs, steps=steps, step_size=step_size, noise=noise\n","    )\n","    self.examples = tf.split(inp_imgs, BATCH_SIZE, axis=0) + self.examples\n","    self.examples = self.examples[:8192]\n","    return inp_imgs"],"metadata":{"id":"vOPhuDgfrIt3","executionInfo":{"status":"ok","timestamp":1683262192431,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class EBM(models.Model):\n","  def __init__(self):\n","    super(EBM, self).__init__()\n","    self.model = model\n","    self.buffer = Buffer(self.model)\n","    self.alpha = ALPHA\n","    self.loss_metric = metrics.Mean(name=\"loss\")\n","    self.reg_loss_metric = metrics.Mean(name=\"reg\")\n","    self.cdiv_loss_metric = metrics.Mean(name=\"cdiv\")  \n","    self.real_out_metric = metrics.Mean(name=\"real\")\n","    self.fake_out_metric = metrics.Mean(name=\"fake\")\n","\n","  @property\n","  def metrics(self):\n","    return [\n","        self.loss_metric,\n","        self.reg_loss_metric,\n","        self.cdiv_loss_metric,\n","        self.real_out_metric,\n","        self.fake_out_metric\n","    ]\n","\n","  def train_step(self, real_imgs):\n","    real_imgs += tf.random.normal(\n","        shape=tf.shape(real_imgs), mean=0, stddev=NOISE\n","    )\n","    real_imgs = tf.clip_by_value(real_imgs, -1.0, 1.0)\n","    fake_imgs = self.buffer.sample_new_exmps(\n","        steps=STEPS, step_size=STEP_SIZE, noise=NOISE\n","    )\n","    inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\n","    with tf.GradientTape() as training_tape:\n","      real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\n","      cdiv_loss = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(\n","          real_out, axis=0\n","      )\n","      reg_loss = self.alpha * tf.reduce_mean(\n","          real_out**2 + fake_out**2, axis=0\n","      )\n","      loss = cdiv_loss + reg_loss \n","    grads = training_tape.gradient(loss, self.model.trainable_variables)\n","    self.optimizer.apply_gradients(\n","        zip(grads, self.model.trainable_variables)\n","    )\n","    self.loss_metric.update_state(loss)\n","    self.reg_loss_metric.update_state(reg_loss)\n","    self.cdiv_loss_metric.update_state(cdiv_loss)\n","    self.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\n","    self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def test_step(self, real_imgs): \n","      batch_size = real_imgs.shape[0]\n","      fake_imgs = tf.random.uniform((batch_size, 32, 32, 1)) * 2 - 1\n","      inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\n","      real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\n","      cdiv = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\n","          real_out, axis = 0\n","      )\n","      self.cdiv_loss_metric.update_state(cdiv)\n","      self.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\n","      self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\n","      return {m.name: m.result() for m in self.metrics[2:]}\n","\n","ebm = EBM()\n","ebm.compile(optimizer=optimizers.Adam(learning_rate=0.0001), run_eagerly=True)\n","ebm.fit(x_train, epochs=60, validation_data = x_test,)"],"metadata":{"id":"JZbDLJYot4E1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8694c43-13ca-4669-a00e-51153ac1b6a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Gk2qA96cvy5H","executionInfo":{"status":"aborted","timestamp":1683262252156,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sayan Dafadar","userId":"01489114010454149167"}}},"execution_count":null,"outputs":[]}]}